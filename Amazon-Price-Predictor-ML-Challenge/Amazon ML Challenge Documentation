Smart Product Pricing Challenge: Project Documentation
Methodology & Approach: Classical Machine Learning Pipeline
1. Project Goal & Model Architecture
The primary goal was to predict product price using the Symmetric Mean Absolute Percentage Error (SMAPE) metric. The solution employs a Classical Machine Learning Pipeline using extensive Feature Engineering to extract signals from messy text and a powerful XGBoost Regressor for prediction.
Model Architecture
Target Variable (y): Log-Transformed Price ($\text{log}(price + 1)$)
Model: XGBoost Regressor (Robust for mixed-type and sparse data).
Final Prediction: The model is trained on 550 features (Flags + OHE + TF-IDF) and the final prediction is inverted using $\text{np.expm1}$.
2. Methodology and Feature Engineering
The solution focuses on Feature Engineering to convert three complex, unstructured columns (catalog_content, Item_Name, Bullet_Points_Combined) into usable numerical features.
Feature Engineering Step
Rationale
Code Implementation Method
Raw Text Parsing
Extracted Item_Name, Value, and Unit using conditional string parsing, mitigating issues from newlines and unstructured text.
robust_parse_catalog_content (Conditional string splitting and regex cleaning).
Unit Standardization
Corrected fragmentation in the Unit column (e.g., 'oz', 'Ounce', 'fl. oz') into uniform categories (e.g., 'ounce', 'fluid ounce').
clean_unit_column (Case normalization and fixed dictionary mapping).
TF-IDF Vectorization
Generated $\sim 500$ predictive features from the combined text, capturing overall linguistic context and complex feature descriptions.
TfidfVectorizer (Max features limited to 500 for resource safety).
Target Transformation
Addressed severe right-skewness in price using the natural logarithm.
$\text{np.log1p}(\text{df['price']})$

3. Exploratory Data Analysis (EDA) Summary
The following is the self-documented EDA conducted to understand the data characteristics and validate the necessary transformations.
A. Univariate Analysis (Data Distribution)
# --- UNIVARIATE ANALYSIS ---

# NUMERICAL ANALYSIS
# Distribution of Target Variable: 'price' (log transformed for a normalized output since its highly skewed)
sns.histplot(np.log1p(train_df['price']), bins=100, kde=True)
plt.title('Log-Transformed Distribution of Prices (NORMALIZED)')
plt.xlabel('Log (Price + 1)')
plt.ylabel('Frequency')
# FINDING: Confirmed severe right-skewness; transformation required.
# Result: Distribution is now approximately normal (bell-shaped).

# Distribution of Variable: 'Value' (Quantity/Sizes)
# FINDING: This column was also found to be highly skewed and requires log transformation.
# Result: Boxplot confirmed a clustered median with high-value outliers.

# OUTLIER Checking with Boxplot
# Result: Boxplots on log-transformed data showed a clustered central distribution
# with clear outliers on the high end, which are retained due to their importance to price prediction.

B. Categorical Analysis (Unit Feature)
# CATEGORICAL ANALYSIS (UNIT COLUMN)

# BARPLOT Chart for Unit Column (Log Scale used to show small categories)
# Result: Initially showed severe fragmentation (e.g., 'Oz', 'ounce', 'Fl Oz').
# The final plot on a log scale confirmed 'ounce' and 'count/pack/unit' are the dominant categories.
# The low-frequency 'other/junk' category was successfully marginalized.

# TABLE View for Unit Column (Quantifies dominance)
# Result: The top 3 clean categories account for over 98% of the data, proving the standardization succeeded.

C. Bivariate Analysis (Correlation)
# BIVARIATE ANALYSIS
# Numerical vs Numerical (Price vs. Value and sample_id)
# Result: Correlation Matrix showed r=0.07 between Value and price.
# FINDING: Value is NOT a strong linear predictor of price. This proved that predictive
# power must reside in the extracted text features (TF-IDF/Flags).

4. Limitations and Model Performance
A. Critical Resource Limitation
The project was limited by CPU resources (Celeron laptop), forcing the abandonment of high-performance V-LM (Vision-Language) models and full $5,000$-feature TF-IDF, leading to a reliance on a safe $\mathbf{500}$-feature sparse matrix pipeline.
B. Model Performance (Validation SMAPE: 57.29%)
The $57.29\%$ Validation SMAPE score is high because the model lacks the true predictive power from Image Data and Brand Recognition, confirming that price is driven by factors not available in the current feature set. The model is technically sound, but the feature set is incomplete.

